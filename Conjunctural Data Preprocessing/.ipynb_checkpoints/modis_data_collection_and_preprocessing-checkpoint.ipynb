{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29850d78",
   "metadata": {},
   "source": [
    "# NDVI Data Collection and Preprocessing\n",
    "\n",
    "NDVI stands for Normalized Difference Vegetation Index(NDVI), a key tool in assesing vegetation health. It's a simple numeric indicator that uses satellite imagery to assess whether the observed target (usually vegetation) contains live green vegetation or not. NDVI is calculated from the visible and near-infrared light reflected by vegetation, involves comparing the reflectance of near-infrared (NIR) and red light.\n",
    "\n",
    "$$ \\text{NDVI} = \\frac{\\text{NIR} - \\text{Red}}{\\text{NIR} + \\text{Red}} $$\n",
    "\n",
    "**Where**\n",
    "\n",
    "- $NIR$ is the near-infrared radiation, and\n",
    "- $Red$ is the visible red radiation.\n",
    "\n",
    "NDVI plays a critical role in food security by enabling various aspects of agricultural management and prediction. It aids in monitoring crop health and growth stages, facilitating timely interventions to optimize agricultural practices. Moreover, decreases in NDVI serve as early indicators of drought stress, warning of potential crop failures due to water scarcity. NDVI data also correlate with crop yields, providing valuable insights for yield estimation and informing food production forecasts. Additionally, NDVI serves as a foundational input for predictive models assessing crucial food security indicators such as Food Consumption Score (FCS) and Household Dietary Diversity Score (HDDS). Furthermore, by identifying areas with healthy vegetation and those under stress, NDVI guides land use planning, optimizing agricultural land allocation and conservation efforts. Overall, integrating NDVI data into food security assessments enhances our ability to predict, monitor, and address vulnerabilities in food systems, supporting evidence-based decision-making for sustainable development goals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09e5661",
   "metadata": {},
   "source": [
    "## About Modis (need to modify this later on)\n",
    "\n",
    "MODIS (or Moderate Resolution Imaging Spectroradiometer) is a key instrument aboard the Terra (originally known as EOS AM-1) and Aqua (originally known as EOS PM-1) satellites. Terra's orbit around the Earth is timed so that it passes from north to south across the equator in the morning, while Aqua passes south to north over the equator in the afternoon. Terra MODIS and Aqua MODIS are viewing the entire Earth's surface every 1 to 2 days, acquiring data in 36 spectral bands, or groups of wavelengths (see MODIS Technical Specifications). These data will improve our understanding of global dynamics and processes occurring on the land, in the oceans, and in the lower atmosphere. MODIS is playing a vital role in the development of validated, global, interactive Earth system models able to predict global change accurately enough to assist policy makers in making sound decisions concerning the protection of our environment.\n",
    "\n",
    "**Data Product Citations:**\n",
    "Didan, K. (2021). MODIS/Terra Vegetation Indices 16-Day L3 Global 250m SIN Grid V061. NASA EOSDIS Land Processes Distributed Active Archive Center. Accessed 2024-05-03 from https://doi.org/10.5067/MODIS/MOD13Q1.061. Accessed May 3, 2024.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929513b1",
   "metadata": {},
   "source": [
    "## About AρρEEARS  (need to modify this later on) \n",
    "Application for Extracting and Exploring Analysis Ready Samples (AρρEEARS)\n",
    "\n",
    "The Application for Extracting and Exploring Analysis Ready Samples (AρρEEARS) offers a simple and efficient way to access and transform geospatial data from a variety of federal data archives. AρρEEARS enables users to subset geospatial datasets using spatial, temporal, and band/layer parameters. Two types of sample requests are available: point samples for geographic coordinates and area samples for spatial areas via vector polygons. Sample requests submitted to AρρEEARS provide users not only with data values, but also associated quality data values. Interactive visualizations with summary statistics are provided for each sample within the application, which allow users to preview and interact with their samples before downloading their data. Get started with a sample request using the Extract option above, or visit the Help page to learn more. \n",
    "\n",
    "\n",
    "**Software Citation:**\n",
    "\n",
    "AppEEARS Team. (2024). Application for Extracting and Exploring Analysis Ready Samples (AppEEARS). Ver. 3.53. NASA EOSDIS Land Processes Distributed Active Archive Center (LP DAAC), USGS/Earth Resources Observation and Science (EROS) Center, Sioux Falls, South Dakota, USA. Accessed May 3, 2024. https://appeears.earthdatacloud.nasa.gov\n",
    "\n",
    "\n",
    "[AppEEARS API](https://appeears.earthdatacloud.nasa.gov/api/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5959d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install eccodes\n",
    "#!pip install xarray\n",
    "#!pip install rioxarray\n",
    "#!pip install netCDF4\n",
    "#!pip install ecmwflibs\n",
    "#!pip install geopandas\n",
    "#!pip install cftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dd537f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray as rx\n",
    "from scipy.io import netcdf\n",
    "from netCDF4 import Dataset\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import numpy as np\n",
    "import cftime\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.exposure\n",
    "import seaborn as sns\n",
    "import geogif\n",
    "from IPython.display import Image\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d9cb19",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e521c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_appeears_products():\n",
    "    \n",
    "    '''\n",
    "    Retrieve and print a list of products available in the AppEEARS API.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Example:\n",
    "        list_appeears_products()\n",
    "    '''\n",
    "    url = 'https://appeears.earthdatacloud.nasa.gov/api/product'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        products = response.json()\n",
    "        if products:\n",
    "            for product in products:\n",
    "                print(f\"Product ID: {product['ProductAndVersion']}, Description: {product['Description']}\")\n",
    "        else:\n",
    "            print(\"Failed to retrieve product list.\")\n",
    "    else:\n",
    "        print(f\"Failed to list products: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "584f9de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data_fetching(geojson_file, years, product_id, layer_name, dest_dir):\n",
    "    \"\"\"\n",
    "    This function allows to submit tasks and fetch data for a range of years.\n",
    "\n",
    "    Parameters:\n",
    "        geojson_file (str): Path to the GeoJSON file.\n",
    "        years (range): Range of years.\n",
    "        product_id (str): Product ID for the data to be fetched.\n",
    "        layer_name (str): Name of the layer.\n",
    "        dest_dir (str): Directory where files will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for year in range(years[0], years[1] + 1):  # Include the end year\n",
    "        start_date = f\"01-01-{year}\"\n",
    "        end_date = f\"12-31-{year}\"\n",
    "        \n",
    "        print(\"Start Fetching the ndvi data for the year :  \", year)\n",
    "        \n",
    "        # Call the function to submit and fetch data for the year\n",
    "        submit_task_and_fetch_data(geojson_file, start_date, end_date, product_id, layer_name, dest_dir)\n",
    "        \n",
    "    print(\"Data collection for all years completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d4343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_appeears_products()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c8692a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NASADEM_NUM': {'AddOffset': '', 'Available': True, 'DataType': 'int16', 'Description': 'Number of Observations', 'Dimensions': ['time', 'lat', 'lon'], 'FillValue': 255, 'FillValueAll': [255], 'Group': '', 'Info': {}, 'IsQA': True, 'Layer': 'NASADEM_NUM', 'OrigDataType': 'int16', 'OrigValidMax': '', 'OrigValidMin': '', 'QualityLayers': '', 'QualityProductAndVersion': '', 'ScaleFactor': '', 'Units': '', 'ValidMax': 255, 'ValidMin': 0, 'XSize': 3601, 'YSize': 3601}}\n"
     ]
    }
   ],
   "source": [
    "#import requests\n",
    "\n",
    "product_id = 'NASADEM_NUMNC.001'\n",
    "response = requests.get('https://appeears.earthdatacloud.nasa.gov/api/product/{0}'.format(product_id))\n",
    "layer_response = response.json()\n",
    "print(layer_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38af144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def netcdf_to_dataframe(netcdf_file):\n",
    "    \n",
    "    '''\n",
    "    Reads a netCDF file and converts its contents into a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        netcdf_file (str): Path to the netCDF file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing the data from the netCDF file.\n",
    "                          Each column represents a variable, and each row represents a time step.\n",
    "    '''\n",
    "    try:\n",
    "        # Open the netCDF file using xarray\n",
    "        ds = xr.open_dataset(netcdf_file,engine=\"netcdf4\")\n",
    "\n",
    "        # Convert the dataset to a pandas DataFrame\n",
    "        df = ds.to_dataframe()\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99d7517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cftime_to_datetime64(cftime_dates):\n",
    "    \"\"\" Convert an array of cftime dates to numpy datetime64.\"\"\"\n",
    "    return np.array([pd.Timestamp(str(date)) for date in cftime_dates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8223684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_nc_file(nc_file, output_dir):\n",
    "    \"\"\"\n",
    "    Splits a yearly NDVI NetCDF file into twelve separate files, each corresponding to one month.\n",
    "    \n",
    "    Parameters:\n",
    "        nc_file (str): Path to the original NetCDF file.\n",
    "        output_dir (str): Directory to save the split NetCDF files.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    ds = xr.open_dataset(nc_file)\n",
    "    total_times = len(ds['time'])\n",
    "    part_size = total_times // 12  # Splitting into twelve parts now\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    file_paths = []\n",
    "    for i in range(12):\n",
    "        start = i * part_size\n",
    "        # Ensure the last part includes any remainder if the total size is not exactly divisible by 12\n",
    "        end = (i + 1) * part_size if i < 11 else total_times\n",
    "        subset = ds.isel(time=slice(start, end))\n",
    "        part_file = f\"{output_dir}/part_{i+1}.nc\"\n",
    "        subset.to_netcdf(part_file)\n",
    "        file_paths.append(part_file)\n",
    "    \n",
    "    ds.close()\n",
    "    return file_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1a4656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ndvi_data(df):\n",
    "    \"\"\"\n",
    "    Processes a DataFrame to compute the mean NDVI and VI Quality grouped by region or province, district, and date.\n",
    "    \"\"\"\n",
    "    # Check if 'time' is in cftime format and convert\n",
    "    if isinstance(df.iloc[0]['time'], cftime._cftime.DatetimeJulian):\n",
    "        df['time'] = convert_cftime_to_datetime64(df['time'])\n",
    "    else:\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "    # Extract year and date (month-day) from 'time'\n",
    "    df['year'] = df['time'].dt.year\n",
    "    df['date'] = df['time'].dt.strftime('%m-%d')  # Format as month-day\n",
    "\n",
    "    # Group by 'province', 'district', 'year', and 'date' and calculate mean\n",
    "    grouped = df.groupby(['region', 'district', 'year', 'date'])\n",
    "    #result_df = grouped[['_250m_16_days_NDVI', '_250m_16_days_VI_Quality']].mean().reset_index()\n",
    "    result_df = grouped[['_500m_16_days_NDVI', '_500m_16_days_VI_Quality']].mean().reset_index()\n",
    "\n",
    "    # Rename columns to shorter names\n",
    "    result_df.rename(columns={\n",
    "        '_500m_16_days_NDVI': 'NDVI',\n",
    "        '_500m_16_days_VI_Quality': 'VI_Quality'\n",
    "    }, inplace=True)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71ed7343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data_extraction(nc_files, shapefile, output_directory):\n",
    "    \n",
    "    '''\n",
    "    Process multiple NetCDF files, extract weather data, compute statistics for each district,\n",
    "    and save the results to separate CSV files. Then merge all files into a single CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        nc_files (list): List of paths to the NetCDF files.\n",
    "        shapefile (str): Path to the shapefile.\n",
    "        output_directory (str): Path to the directory where the output CSV files will be saved.\n",
    "    '''\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Iterate over each NetCDF file\n",
    "    for nc_file in nc_files:\n",
    "        # Extract weather data from the NetCDF file\n",
    "        ndvi_data = extract_ndvi_data(nc_file, shapefile)\n",
    "\n",
    "        # Compute district ndvi mean statistics\n",
    "        district_stats = process_ndvi_data(ndvi_data)\n",
    "\n",
    "        # Generate output file name based on the year column\n",
    "        year = district_stats['year'].iloc[1]  # Assuming all rows have the same year\n",
    "\n",
    "        output_file = os.path.join(output_directory, f\"ndvi_{year}.csv\")\n",
    "\n",
    "        # Save the district statistics to CSV\n",
    "        district_stats.to_csv(output_file, index=False)\n",
    "        print(f\"The NDVI Data for year {year} has been successfully extracted and saved to:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20f454f",
   "metadata": {},
   "source": [
    "## Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0081dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import imageio\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display  # For displaying GIFs in Jupyter notebooks\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "def create_ndvi_geogif(nc_file_path, output_gif_path=None):\n",
    "    \"\"\"\n",
    "    Creates an animated GIF of NDVI variations from an xarray Dataset loaded from a NetCDF file.\n",
    "\n",
    "    Parameters:\n",
    "        nc_file_path (str): Path to the NetCDF file containing NDVI data.\n",
    "        output_gif_path (str, optional): Path to save the output GIF file. If None, the GIF is displayed.\n",
    "    \"\"\"\n",
    "    # Load the dataset with time decoding turned off initially\n",
    "    ds = xr.open_dataset(nc_file_path, decode_times=False)\n",
    "\n",
    "    # Convert time values assuming they are days since '2000-01-01'\n",
    "    base_date = pd.Timestamp('2000-01-01')\n",
    "    ds['time'] = pd.to_datetime(ds['time'].values, unit='D', origin=base_date)\n",
    "\n",
    "    # Hard-coded variable name for NDVI\n",
    "    #var_name = '_250m_16_days_NDVI' # Rwanda\n",
    "    var_name = '_500m_16_days_NDVI' #Tanzania\n",
    "\n",
    "    # Check if the NDVI variable exists in the dataset\n",
    "    if var_name not in ds.variables:\n",
    "        raise ValueError(f\"{var_name} not found in the dataset\")\n",
    "\n",
    "    # Extract the NDVI DataArray\n",
    "    ndvi = ds[var_name]\n",
    "\n",
    "    # Create the frames for the GIF\n",
    "    frames = []\n",
    "    for i in range(len(ndvi.time)):\n",
    "        fig, ax = plt.subplots()\n",
    "        ndvi.isel(time=i).plot(ax=ax, cmap='viridis')\n",
    "        ax.set_title(f\"NDVI on {pd.to_datetime(ndvi.time.values[i]).strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        # Save the frame to a temporary file\n",
    "        with NamedTemporaryFile(delete=False, suffix='.png') as tmpfile:\n",
    "            fig.savefig(tmpfile.name)\n",
    "            frames.append(imageio.imread(tmpfile.name))\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Create and save the GIF\n",
    "    if output_gif_path:\n",
    "        imageio.mimsave(output_gif_path, frames, fps=1)\n",
    "        print(f\"GIF saved to {output_gif_path}\")\n",
    "    else:\n",
    "        # Display the GIF inline in Jupyter or similar environment\n",
    "        with NamedTemporaryFile(delete=False, suffix='.gif') as tmpfile:\n",
    "            imageio.mimsave(tmpfile.name, frames, fps=1)\n",
    "            display(Image(filename=tmpfile.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e52037",
   "metadata": {},
   "source": [
    "## Fetch ndvi Data From AppEEARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a2db5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_task_and_fetch_data(geojson_file, start_date, end_date, product_id, layer_name, dest_dir):\n",
    "    \"\"\"\n",
    "    Submit a task to fetch data using AppEEARS API and save the downloaded files in a directory based on the year.\n",
    "\n",
    "    Args:\n",
    "        geojson_file (str): Path to the GeoJSON file.\n",
    "        start_date (str): Start date in MM-DD-YYYY format.\n",
    "        end_date (str): End date in MM-DD-YYYY format.\n",
    "        product_id (str): Product ID for the data to be fetched.\n",
    "        layer_name (str): Name of the layer.\n",
    "        dest_dir (str): Directory where files will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Example:\n",
    "        submit_task_and_fetch_data('path/to/geojson_file.geojson', '01-01-2024', '12-31-2024', 'product_id', 'layer_name', 'destination_directory')\n",
    "    \"\"\"\n",
    "    # Authenticate\n",
    "    auth_url = 'https://appeears.earthdatacloud.nasa.gov/api/login'\n",
    "    credentials = ('ashuu944', 'Hassan@16219')  # Use environment variables in production\n",
    "    response = requests.post(auth_url, auth=credentials)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Authentication failed.\")\n",
    "        return\n",
    "    token = response.json()['token']\n",
    "\n",
    "    # Read the GeoJSON file\n",
    "    with open(geojson_file, 'r') as file:\n",
    "        geojson = json.load(file)\n",
    "\n",
    "    # Define the URL for submitting a task\n",
    "    task_url = 'https://appeears.earthdatacloud.nasa.gov/api/task'\n",
    "    headers = {'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'}\n",
    "    task_params = {\n",
    "        \"task_type\": \"area\",\n",
    "        \"task_name\": \"NDVI Data Extraction\",\n",
    "        \"params\": {\n",
    "            \"dates\": [\n",
    "                {\n",
    "                    \"startDate\": start_date,\n",
    "                    \"endDate\": end_date,\n",
    "                    \"recurring\": False\n",
    "                }\n",
    "            ],\n",
    "            \"layers\": [\n",
    "                {\n",
    "                    \"product\": product_id,\n",
    "                    \"layer\": layer_name\n",
    "                }\n",
    "            ],\n",
    "            \"output\": {\n",
    "                \"format\": {\n",
    "                    \"type\": \"netcdf4\"\n",
    "                },\n",
    "                \"projection\": \"geographic\"\n",
    "            },\n",
    "            \"geo\": geojson\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Submit the task\n",
    "    task_response = requests.post(task_url, headers=headers, json=task_params)\n",
    "    task_id = task_response.json()['task_id']\n",
    "    print(f\"Task submitted, ID: {task_id}\")\n",
    "\n",
    "    # Check task status until it is done\n",
    "    while True:\n",
    "        time.sleep(50)  # Wait for 50 seconds before checking again\n",
    "        status_response = requests.get(f'{task_url}/{task_id}', headers=headers)\n",
    "        task_status = status_response.json()['status']\n",
    "        print(f\"Current task status: {task_status}\")\n",
    "        if task_status == 'done':\n",
    "            print(\"Task completed. Proceeding to fetch file list.\")\n",
    "            break\n",
    "        elif task_status in ['error', 'failed']:\n",
    "            print(\"Task failed.\")\n",
    "            return\n",
    "\n",
    "    # Fetch the bundle to get file IDs\n",
    "    bundle_url = f'https://appeears.earthdatacloud.nasa.gov/api/bundle/{task_id}'\n",
    "    bundle_response = requests.get(bundle_url, headers={'Authorization': f'Bearer {token}'})\n",
    "    if bundle_response.status_code == 200:\n",
    "        files = bundle_response.json()['files']\n",
    "        # Get the year from start_date\n",
    "        year = datetime.strptime(start_date, '%m-%d-%Y').year\n",
    "        year_dir = os.path.join(dest_dir, str(year))\n",
    "        os.makedirs(year_dir, exist_ok=True)\n",
    "        for file in files:\n",
    "            file_id = file['file_id']\n",
    "            file_name = file['file_name']\n",
    "            file_download_url = f'https://appeears.earthdatacloud.nasa.gov/api/bundle/{task_id}/{file_id}'\n",
    "            download_response = requests.get(file_download_url, headers={'Authorization': f'Bearer {token}'}, stream=True)\n",
    "\n",
    "            if download_response.status_code == 200:\n",
    "                # Prefix the file name with the year\n",
    "                year_prefixed_file_name = f\"{year}_{file_name}\"\n",
    "                filepath = os.path.join(year_dir, year_prefixed_file_name)\n",
    "                with open(filepath, 'wb') as f:\n",
    "                    for chunk in download_response.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                print(f\"Data downloaded successfully and saved to {filepath}\")\n",
    "            else:\n",
    "                print(f\"Failed to download file {file_name}: {download_response.status_code}, {download_response.text}\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch file list: {bundle_response.status_code}, {bundle_response.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1851d88",
   "metadata": {},
   "source": [
    "## Extract the Data As Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dda41fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ndvi_data(nc_file, shapefile):\n",
    "    \"\"\"\n",
    "    Extracts NDVI data from a NetCDF file for each geometry in a shapefile, excluding rows with null NDVI values.\n",
    "\n",
    "    Parameters:\n",
    "        nc_file (str): Path to the NetCDF file.\n",
    "        shapefile (str): Path to the shapefile.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing extracted NDVI data, excluding null NDVI values.\n",
    "    \"\"\"\n",
    "    # Load the NetCDF file with xarray\n",
    "    ds = xr.open_dataset(nc_file)\n",
    "    ds.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\", inplace=True)\n",
    "\n",
    "    # Manually set the CRS if not defined\n",
    "    if ds.rio.crs is None:\n",
    "        ds.rio.write_crs(\"epsg:4326\", inplace=True)\n",
    "\n",
    "    # Load the shapefile with GeoPandas\n",
    "    gdf = gpd.read_file(shapefile)\n",
    "\n",
    "    # Convert GeoDataFrame CRS to match Dataset CRS\n",
    "    if gdf.crs != ds.rio.crs:\n",
    "        gdf = gdf.to_crs(ds.rio.crs)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Determine the column name for administrative regions dynamically\n",
    "    region_column_name = None\n",
    "    for column_name in ['region', 'province']:\n",
    "        if column_name in gdf.columns:\n",
    "            region_column_name = column_name\n",
    "            break\n",
    "\n",
    "    # Loop through each geometry in the GeoDataFrame\n",
    "    for idx, row in gdf.iterrows():\n",
    "        geometry = row.geometry\n",
    "\n",
    "        # Clip the dataset by the geometry\n",
    "        clipped = ds.rio.clip([geometry], gdf.crs, drop=True, all_touched=True)\n",
    "\n",
    "        # Process each clipped dataset\n",
    "        if 'time' in clipped.dims:\n",
    "            # Convert to DataFrame\n",
    "            df = clipped.to_dataframe().reset_index()\n",
    "            \n",
    "            # Filter out rows where '_250m_16_days_NDVI' is null\n",
    "            '''\n",
    "            if '_250m_16_days_NDVI' in df.columns:\n",
    "                df = df.dropna(subset=['_250m_16_days_NDVI'])\n",
    "            '''\n",
    "            # Filter out rows where '_500m_16_days_NDVI' is null\n",
    "            if '_500m_16_days_NDVI' in df.columns:\n",
    "                df = df.dropna(subset=['_500m_16_days_NDVI'])\n",
    "\n",
    "            # Continue only if there are remaining rows after filtering\n",
    "            if not df.empty:\n",
    "                # Add region or province column\n",
    "                if region_column_name:\n",
    "                    df[region_column_name] = row[region_column_name]\n",
    "                    df['district'] = row['district']  # assuming 'district' column always exists\n",
    "                    results.append(df)\n",
    "                else:\n",
    "                    print(\"No administrative region column found in the shapefile. Skipping...\")\n",
    "            else:\n",
    "                print(f\"No NDVI data for geometry at index {idx}. Skipping...\")\n",
    "\n",
    "        else:\n",
    "            print(f\"No data for geometry at index {idx} in the dataset.\")\n",
    "\n",
    "    # Combine all dataframes into a single DataFrame\n",
    "    if results:\n",
    "        result_df = pd.concat(results, ignore_index=True)\n",
    "    else:\n",
    "        result_df = pd.DataFrame()  # Return an empty DataFrame if no results\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2eca8a",
   "metadata": {},
   "source": [
    "## Download Tanzania Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "806ac25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "geojson_file = 'tanzania_data/shapefiles/tz_country.geojson'\n",
    "years = (2009, 2023)\n",
    "#start_date = '01-01-2008'\n",
    "#end_date   = '12-31-2008'\n",
    "#product_id = 'MOD13Q1.061' # 250m resolution product\n",
    "#layer_name = '_250m_16_days_NDVI' # 250m resolution layer\n",
    "\n",
    "product_id = 'MOD13A1.061' # 500m resolution product\n",
    "layer_name = '_500m_16_days_NDVI' # 500m resolution layer\n",
    "\n",
    "dest_dir = \"tanzania_data/ndvi_data/\"\n",
    "#submit_task_and_fetch_data(geojson_file, start_date, end_date, product_id, layer_name, dest_dir)\n",
    "#batch_data_fetching(geojson_file,years,product_id,layer_name,dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b20744",
   "metadata": {},
   "source": [
    "## Extra Data for each Districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03188489",
   "metadata": {},
   "outputs": [],
   "source": [
    "tz_dir = 'tanzania_data/ndvi_data/'\n",
    "#nc_2008 = tz_dir+ '2008/2008_MOD13A1.061_500m_aid0001.nc'\n",
    "nc_2009 = tz_dir+ '2009/2009_MOD13A1.061_500m_aid0001.nc'\n",
    "nc_2010 = tz_dir+ '2010/2010_MOD13A1.061_500m_aid0001.nc'\n",
    "nc_2011 = tz_dir+ '2011/2011_MOD13A1.061_500m_aid0001.nc'\n",
    "nc_2012 = tz_dir+ '2012/2012_MOD13A1.061_500m_aid0001.nc'\n",
    "nc_2013 = tz_dir+ '2013/2013_MOD13A1.061_500m_aid0001.nc'\n",
    "nc_2014 = tz_dir+ '2014/2014_MOD13A1.061_500m_aid0001.nc'\n",
    "nc_2015 = tz_dir+ '2015/2015_MOD13A1.061_500m_aid0001.nc'\n",
    "nc_2016 = tz_dir+ '2016/2016_MOD13A1.061_500m_aid0001.nc'\n",
    "nc_2017 = tz_dir+ '2017/2017_MOD13A1.061_500m_aid0001.nc'\n",
    "nc_2018 = tz_dir+ '2018/2018_MOD13A1.061_500m_aid0001.nc'\n",
    "nc_2019 = tz_dir+ '2019/2019_MOD13A1.061_500m_aid0001.nc'\n",
    "nc_2020 = tz_dir+ '2020/2020_MOD13A1.061_500m_aid0001.nc'\n",
    "nc_2021 = tz_dir+ '2021/2021_MOD13A1.061_500m_aid0001.nc'\n",
    "nc_2022 = tz_dir+ '2022/2022_MOD13A1.061_500m_aid0001.nc'\n",
    "nc_2023 = tz_dir+ '2023/2023_MOD13A1.061_500m_aid0001.nc'\n",
    "nc_files = [nc_2016, nc_2017, nc_2018, nc_2019, nc_2020, nc_2021, nc_2022, nc_2023]\n",
    "shapefile = 'tanzania_data/shapefiles/tz_districts.shp'\n",
    "output_directory = tz_dir + 'processed/'\n",
    "#batch_data_extraction(nc_files, shapefile, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6fb7c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashas\\AppData\\Local\\Temp\\ipykernel_15236\\2845498676.py:44: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  frames.append(imageio.imread(tmpfile.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF saved to tanzania_data/ndvi_data/processed/ndvi.gif\n"
     ]
    }
   ],
   "source": [
    "create_ndvi_geogif(nc_2010,output_directory+'ndvi.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b595ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tanzania_data/ndvi_data/processed/ndvi_2023.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19237eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>district</th>\n",
       "      <th>year</th>\n",
       "      <th>date</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>VI_Quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4435</th>\n",
       "      <td>Tanga</td>\n",
       "      <td>Tanga</td>\n",
       "      <td>2023</td>\n",
       "      <td>10-16</td>\n",
       "      <td>0.593749</td>\n",
       "      <td>6833.893521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4436</th>\n",
       "      <td>Tanga</td>\n",
       "      <td>Tanga</td>\n",
       "      <td>2023</td>\n",
       "      <td>11-01</td>\n",
       "      <td>0.565007</td>\n",
       "      <td>7177.000321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4437</th>\n",
       "      <td>Tanga</td>\n",
       "      <td>Tanga</td>\n",
       "      <td>2023</td>\n",
       "      <td>11-17</td>\n",
       "      <td>0.710040</td>\n",
       "      <td>4246.743377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4438</th>\n",
       "      <td>Tanga</td>\n",
       "      <td>Tanga</td>\n",
       "      <td>2023</td>\n",
       "      <td>12-03</td>\n",
       "      <td>0.740884</td>\n",
       "      <td>3487.930946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4439</th>\n",
       "      <td>Tanga</td>\n",
       "      <td>Tanga</td>\n",
       "      <td>2023</td>\n",
       "      <td>12-19</td>\n",
       "      <td>0.715521</td>\n",
       "      <td>4403.707474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     region district  year   date      NDVI   VI_Quality\n",
       "4435  Tanga    Tanga  2023  10-16  0.593749  6833.893521\n",
       "4436  Tanga    Tanga  2023  11-01  0.565007  7177.000321\n",
       "4437  Tanga    Tanga  2023  11-17  0.710040  4246.743377\n",
       "4438  Tanga    Tanga  2023  12-03  0.740884  3487.930946\n",
       "4439  Tanga    Tanga  2023  12-19  0.715521  4403.707474"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2a3d54",
   "metadata": {},
   "source": [
    "## Rwanda Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "988eafff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "geojson_file = 'rwanda_data/shapefiles/rw_country.geojson'\n",
    "#years = (2012, 2021)\n",
    "start_date = '01-01-2005'\n",
    "end_date   = '12-31-2005'\n",
    "product_id = 'MOD13Q1.061'\n",
    "layer_name = '_250m_16_days_NDVI'\n",
    "dest_dir = \"rwanda_data/ndvi_data/\"\n",
    "#submit_task_and_fetch_data(geojson_file, start_date, end_date, product_id, layer_name, dest_dir)\n",
    "#batch_data_fetching(geojson_file,years,product_id,layer_name,dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c06d94",
   "metadata": {},
   "source": [
    "## Extra Data for each Districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51de8044",
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_dir = 'rwanda_data/ndvi_data/'\n",
    "nc_2006 = rw_dir+ '2006/2006_MOD13Q1.061_250m_aid0001.nc'\n",
    "nc_2012 = rw_dir+ '2012/2012_MOD13Q1.061_250m_aid0001.nc'\n",
    "nc_2015 = rw_dir+ '2015/2015_MOD13Q1.061_250m_aid0001.nc'\n",
    "nc_2018 = rw_dir+ '2018/2018_MOD13Q1.061_250m_aid0001.nc'\n",
    "nc_2021 = rw_dir+ '2021/2021_MOD13Q1.061_250m_aid0001.nc'\n",
    "nc_files = [nc_2006, nc_2012, nc_2015, nc_2018, nc_2021]\n",
    "shapefile = 'rwanda_data/shapefiles/rw_district.shp'\n",
    "output_directory = rw_dir + 'processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc66d241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashas\\AppData\\Local\\Temp\\ipykernel_15236\\1262916630.py:43: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  frames.append(imageio.imread(tmpfile.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF saved to rwanda_data/ndvi_data/processed/ndvi.gif\n"
     ]
    }
   ],
   "source": [
    "create_ndvi_geogif(nc_2006,output_directory+'ndvi.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01ac1b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_data_extraction(nc_files, shapefile, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4401a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('rwanda_data/ndvi_data/processed/ndvi_2006.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0de495c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>province</th>\n",
       "      <th>district</th>\n",
       "      <th>year</th>\n",
       "      <th>date</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>VI_Quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amajyaruguru</td>\n",
       "      <td>Burera</td>\n",
       "      <td>2005</td>\n",
       "      <td>12-19</td>\n",
       "      <td>0.434430</td>\n",
       "      <td>9949.981146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amajyaruguru</td>\n",
       "      <td>Burera</td>\n",
       "      <td>2006</td>\n",
       "      <td>01-01</td>\n",
       "      <td>0.373703</td>\n",
       "      <td>8616.219062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amajyaruguru</td>\n",
       "      <td>Burera</td>\n",
       "      <td>2006</td>\n",
       "      <td>01-17</td>\n",
       "      <td>0.518290</td>\n",
       "      <td>2877.326568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amajyaruguru</td>\n",
       "      <td>Burera</td>\n",
       "      <td>2006</td>\n",
       "      <td>02-02</td>\n",
       "      <td>0.465203</td>\n",
       "      <td>3139.811856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amajyaruguru</td>\n",
       "      <td>Burera</td>\n",
       "      <td>2006</td>\n",
       "      <td>02-18</td>\n",
       "      <td>0.503937</td>\n",
       "      <td>4595.015340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       province district  year   date      NDVI   VI_Quality\n",
       "0  Amajyaruguru   Burera  2005  12-19  0.434430  9949.981146\n",
       "1  Amajyaruguru   Burera  2006  01-01  0.373703  8616.219062\n",
       "2  Amajyaruguru   Burera  2006  01-17  0.518290  2877.326568\n",
       "3  Amajyaruguru   Burera  2006  02-02  0.465203  3139.811856\n",
       "4  Amajyaruguru   Burera  2006  02-18  0.503937  4595.015340"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eedd3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
