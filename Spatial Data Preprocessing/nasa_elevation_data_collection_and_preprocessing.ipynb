{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29850d78",
   "metadata": {},
   "source": [
    "# Elevation Data Collection and Preprocessing\n",
    "\n",
    "need to write something here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09e5661",
   "metadata": {},
   "source": [
    "## About Modis (need to modify this later on)\n",
    "\n",
    "MODIS (or Moderate Resolution Imaging Spectroradiometer) is a key instrument aboard the Terra (originally known as EOS AM-1) and Aqua (originally known as EOS PM-1) satellites. Terra's orbit around the Earth is timed so that it passes from north to south across the equator in the morning, while Aqua passes south to north over the equator in the afternoon. Terra MODIS and Aqua MODIS are viewing the entire Earth's surface every 1 to 2 days, acquiring data in 36 spectral bands, or groups of wavelengths (see MODIS Technical Specifications). These data will improve our understanding of global dynamics and processes occurring on the land, in the oceans, and in the lower atmosphere. MODIS is playing a vital role in the development of validated, global, interactive Earth system models able to predict global change accurately enough to assist policy makers in making sound decisions concerning the protection of our environment.\n",
    "\n",
    "**Data Product Citations:**\n",
    "Data Product Citations: \n",
    "NASA JPL (2013). NASA Shuttle Radar Topography Mission Global 3 arc second NetCDF. NASA EOSDIS Land Processes Distributed Active Archive Center. Accessed 2024-05-16 from https://doi.org/10.5067/MEaSUREs/SRTM/SRTMGL3_NC.003. Accessed May 16, 2024. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929513b1",
   "metadata": {},
   "source": [
    "## About AρρEEARS  (need to modify this later on) \n",
    "Application for Extracting and Exploring Analysis Ready Samples (AρρEEARS)\n",
    "\n",
    "The Application for Extracting and Exploring Analysis Ready Samples (AρρEEARS) offers a simple and efficient way to access and transform geospatial data from a variety of federal data archives. AρρEEARS enables users to subset geospatial datasets using spatial, temporal, and band/layer parameters. Two types of sample requests are available: point samples for geographic coordinates and area samples for spatial areas via vector polygons. Sample requests submitted to AρρEEARS provide users not only with data values, but also associated quality data values. Interactive visualizations with summary statistics are provided for each sample within the application, which allow users to preview and interact with their samples before downloading their data. Get started with a sample request using the Extract option above, or visit the Help page to learn more. \n",
    "\n",
    "\n",
    "**Software Citation:**\n",
    "\n",
    "Software Citation: \n",
    "AppEEARS Team. (2024). Application for Extracting and Exploring Analysis Ready Samples (AppEEARS). Ver. 3.54. NASA EOSDIS Land Processes Distributed Active Archive Center (LP DAAC), USGS/Earth Resources Observation and Science (EROS) Center, Sioux Falls, South Dakota, USA. Accessed May 16, 2024. https://appeears.earthdatacloud.nasa.gov \n",
    "\n",
    "\n",
    "\n",
    "[AppEEARS API](https://appeears.earthdatacloud.nasa.gov/api/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5959d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install eccodes\n",
    "#!pip install xarray\n",
    "#!pip install rioxarray\n",
    "#!pip install netCDF4\n",
    "#!pip install ecmwflibs\n",
    "#!pip install geopandas\n",
    "#!pip install cftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dd537f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray as rx\n",
    "from scipy.io import netcdf\n",
    "from netCDF4 import Dataset\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "import numpy as np\n",
    "import cftime\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.exposure\n",
    "import seaborn as sns\n",
    "import geogif\n",
    "from IPython.display import Image\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d9cb19",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e521c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_appeears_products():\n",
    "    \n",
    "    '''\n",
    "    Retrieve and print a list of products available in the AppEEARS API.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Example:\n",
    "        list_appeears_products()\n",
    "    '''\n",
    "    url = 'https://appeears.earthdatacloud.nasa.gov/api/product'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        products = response.json()\n",
    "        if products:\n",
    "            for product in products:\n",
    "                print(f\"Product ID: {product['ProductAndVersion']}, Description: {product['Description']}\")\n",
    "        else:\n",
    "            print(\"Failed to retrieve product list.\")\n",
    "    else:\n",
    "        print(f\"Failed to list products: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85d4343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_appeears_products()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c8692a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SRTMGL3_DEM': {'AddOffset': '', 'Available': True, 'DataType': 'int16', 'Description': 'Elevation', 'Dimensions': ['time', 'lat', 'lon'], 'FillValue': -32768, 'FillValueAll': [-32768], 'Group': '', 'Info': {}, 'IsQA': False, 'Layer': 'SRTMGL3_DEM', 'OrigDataType': 'int16', 'OrigValidMax': 32767, 'OrigValidMin': -32767, 'QualityLayers': \"['SRTMGL3_NUM']\", 'QualityProductAndVersion': 'SRTMGL3_NUMNC.003', 'ScaleFactor': '', 'Units': 'Meters', 'ValidMax': 32767, 'ValidMin': -32767, 'XSize': 1201, 'YSize': 1201}}\n"
     ]
    }
   ],
   "source": [
    "#import requests\n",
    "product_id = 'SRTMGL3_NC.003'\n",
    "response = requests.get('https://appeears.earthdatacloud.nasa.gov/api/product/{0}'.format(product_id))\n",
    "layer_response = response.json()\n",
    "print(layer_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38af144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def netcdf_to_dataframe(netcdf_file):\n",
    "    \n",
    "    '''\n",
    "    Reads a netCDF file and converts its contents into a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        netcdf_file (str): Path to the netCDF file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing the data from the netCDF file.\n",
    "                          Each column represents a variable, and each row represents a time step.\n",
    "    '''\n",
    "    try:\n",
    "        # Open the netCDF file using xarray\n",
    "        ds = xr.open_dataset(netcdf_file,engine=\"netcdf4\")\n",
    "\n",
    "        # Convert the dataset to a pandas DataFrame\n",
    "        df = ds.to_dataframe()\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99d7517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cftime_to_datetime64(cftime_dates):\n",
    "    \"\"\" Convert an array of cftime dates to numpy datetime64.\"\"\"\n",
    "    return np.array([pd.Timestamp(str(date)) for date in cftime_dates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1a4656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats(df, region_column_name):\n",
    "    \"\"\"\n",
    "    Computes the maximum and variance of elevation data by region and district.\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): DataFrame containing elevation data.\n",
    "        region_column_name (str): Column name for the region.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with computed statistics.\n",
    "    \"\"\"\n",
    "    grouped = df.groupby([region_column_name, 'district'])['SRTMGL3_DEM']\n",
    "    return grouped.agg(max_elevation='max', variance_elevation='var').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71ed7343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data_extraction(nc_files, shapefile, output_directory):\n",
    "    \n",
    "    '''\n",
    "    Process multiple NetCDF files, extract weather data, compute statistics for each district,\n",
    "    and save the results to separate CSV files. Then merge all files into a single CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        nc_files (list): List of paths to the NetCDF files.\n",
    "        shapefile (str): Path to the shapefile.\n",
    "        output_directory (str): Path to the directory where the output CSV files will be saved.\n",
    "    '''\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Iterate over each NetCDF file\n",
    "    for nc_file in nc_files:\n",
    "        # Extract weather data from the NetCDF file\n",
    "        ndvi_data = extract_ndvi_data(nc_file, shapefile)\n",
    "\n",
    "        # Compute district ndvi mean statistics\n",
    "        district_stats = process_ndvi_data(ndvi_data)\n",
    "\n",
    "        # Generate output file name based on the year column\n",
    "        year = district_stats['year'].iloc[1]  # Assuming all rows have the same year\n",
    "\n",
    "        output_file = os.path.join(output_directory, f\"ndvi_{year}.csv\")\n",
    "\n",
    "        # Save the district statistics to CSV\n",
    "        district_stats.to_csv(output_file, index=False)\n",
    "        print(f\"The NDVI Data for year {year} has been successfully extracted and saved to:\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e52037",
   "metadata": {},
   "source": [
    "## Fetch Elevation Data From AppEEARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a2db5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_task_and_fetch_data(geojson_file, start_date, end_date, product_id, layer_name, dest_dir):\n",
    "    \"\"\"\n",
    "    Submit a task to fetch data using AppEEARS API and save the downloaded files in a directory based on the year.\n",
    "\n",
    "    Args:\n",
    "        geojson_file (str): Path to the GeoJSON file.\n",
    "        start_date (str): Start date in MM-DD-YYYY format.\n",
    "        end_date (str): End date in MM-DD-YYYY format.\n",
    "        product_id (str): Product ID for the data to be fetched.\n",
    "        layer_name (str): Name of the layer.\n",
    "        dest_dir (str): Directory where files will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Example:\n",
    "        submit_task_and_fetch_data('path/to/geojson_file.geojson', '01-01-2024', '12-31-2024', 'product_id', 'layer_name', 'destination_directory')\n",
    "    \"\"\"\n",
    "    # Authenticate\n",
    "    auth_url = 'https://appeears.earthdatacloud.nasa.gov/api/login'\n",
    "    credentials = ('ashuu944', 'Hassan@16219')  # Use environment variables in production\n",
    "    response = requests.post(auth_url, auth=credentials)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Authentication failed.\")\n",
    "        return\n",
    "    token = response.json()['token']\n",
    "\n",
    "    # Read the GeoJSON file\n",
    "    with open(geojson_file, 'r') as file:\n",
    "        geojson = json.load(file)\n",
    "\n",
    "    # Define the URL for submitting a task\n",
    "    task_url = 'https://appeears.earthdatacloud.nasa.gov/api/task'\n",
    "    headers = {'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'}\n",
    "    task_params = {\n",
    "        \"task_type\": \"area\",\n",
    "        \"task_name\": \"NDVI Data Extraction\",\n",
    "        \"params\": {\n",
    "            \"dates\": [\n",
    "                {\n",
    "                    \"startDate\": start_date,\n",
    "                    \"endDate\": end_date,\n",
    "                    \"recurring\": False\n",
    "                }\n",
    "            ],\n",
    "            \"layers\": [\n",
    "                {\n",
    "                    \"product\": product_id,\n",
    "                    \"layer\": layer_name\n",
    "                }\n",
    "            ],\n",
    "            \"output\": {\n",
    "                \"format\": {\n",
    "                    \"type\": \"netcdf4\"\n",
    "                },\n",
    "                \"projection\": \"geographic\"\n",
    "            },\n",
    "            \"geo\": geojson\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Submit the task\n",
    "    task_response = requests.post(task_url, headers=headers, json=task_params)\n",
    "    task_id = task_response.json()['task_id']\n",
    "    print(f\"Task submitted, ID: {task_id}\")\n",
    "\n",
    "    # Check task status until it is done\n",
    "    while True:\n",
    "        time.sleep(50)  # Wait for 50 seconds before checking again\n",
    "        status_response = requests.get(f'{task_url}/{task_id}', headers=headers)\n",
    "        task_status = status_response.json()['status']\n",
    "        print(f\"Current task status: {task_status}\")\n",
    "        if task_status == 'done':\n",
    "            print(\"Task completed. Proceeding to fetch file list.\")\n",
    "            break\n",
    "        elif task_status in ['error', 'failed']:\n",
    "            print(\"Task failed.\")\n",
    "            return\n",
    "\n",
    "    # Fetch the bundle to get file IDs\n",
    "    bundle_url = f'https://appeears.earthdatacloud.nasa.gov/api/bundle/{task_id}'\n",
    "    bundle_response = requests.get(bundle_url, headers={'Authorization': f'Bearer {token}'})\n",
    "    if bundle_response.status_code == 200:\n",
    "        files = bundle_response.json()['files']\n",
    "        # Get the year from start_date\n",
    "        year = datetime.strptime(start_date, '%m-%d-%Y').year\n",
    "        year_dir = os.path.join(dest_dir, str(year))\n",
    "        os.makedirs(year_dir, exist_ok=True)\n",
    "        for file in files:\n",
    "            file_id = file['file_id']\n",
    "            file_name = file['file_name']\n",
    "            file_download_url = f'https://appeears.earthdatacloud.nasa.gov/api/bundle/{task_id}/{file_id}'\n",
    "            download_response = requests.get(file_download_url, headers={'Authorization': f'Bearer {token}'}, stream=True)\n",
    "\n",
    "            if download_response.status_code == 200:\n",
    "                # Prefix the file name with the year\n",
    "                year_prefixed_file_name = f\"{year}_{file_name}\"\n",
    "                filepath = os.path.join(year_dir, year_prefixed_file_name)\n",
    "                with open(filepath, 'wb') as f:\n",
    "                    for chunk in download_response.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                print(f\"Data downloaded successfully and saved to {filepath}\")\n",
    "            else:\n",
    "                print(f\"Failed to download file {file_name}: {download_response.status_code}, {download_response.text}\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch file list: {bundle_response.status_code}, {bundle_response.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1851d88",
   "metadata": {},
   "source": [
    "## Extract the Elevation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dda41fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_elevation_data(nc_file, shapefile, save_dir):\n",
    "    \"\"\"\n",
    "    Extracts elevation data from a NetCDF file for each geometry in a shapefile,\n",
    "    computes the maximum and variance of the elevation data by district and region,\n",
    "    and saves the results in a CSV file and a spatial data file.\n",
    "\n",
    "    Parameters:\n",
    "        nc_file (str): Path to the NetCDF file.\n",
    "        shapefile (str): Path to the shapefile.\n",
    "        save_dir (str): Directory where the output files will be saved.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing extracted elevation data, excluding null values.\n",
    "    \"\"\"\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Load the NetCDF file with xarray\n",
    "    ds = xr.open_dataset(nc_file)\n",
    "    ds.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\", inplace=True)\n",
    "\n",
    "    # Manually set the CRS if not defined\n",
    "    if ds.rio.crs is None:\n",
    "        ds.rio.write_crs(\"epsg:4326\", inplace=True)\n",
    "\n",
    "    # Load the shapefile with GeoPandas\n",
    "    gdf = gpd.read_file(shapefile)\n",
    "\n",
    "    # Convert GeoDataFrame CRS to match Dataset CRS\n",
    "    if gdf.crs != ds.rio.crs:\n",
    "        gdf = gdf.to_crs(ds.rio.crs)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Determine the column name for administrative regions dynamically\n",
    "    region_column_name = None\n",
    "    for column_name in ['region', 'province']:\n",
    "        if column_name in gdf.columns:\n",
    "            region_column_name = column_name\n",
    "            break\n",
    "\n",
    "    if not region_column_name:\n",
    "        raise ValueError(\"No administrative region column found in the shapefile.\")\n",
    "\n",
    "    # Loop through each geometry in the GeoDataFrame\n",
    "    for idx, row in gdf.iterrows():\n",
    "        geometry = row.geometry\n",
    "\n",
    "        # Clip the dataset by the geometry\n",
    "        clipped = ds.rio.clip([geometry], gdf.crs, drop=True, all_touched=True)\n",
    "\n",
    "        # Process each clipped dataset\n",
    "        if 'time' in clipped.dims:\n",
    "            # Convert to DataFrame\n",
    "            df = clipped.to_dataframe().reset_index()\n",
    "\n",
    "            # Filter out rows where 'SRTMGL3_DEM' is null\n",
    "            if 'SRTMGL3_DEM' in df.columns:\n",
    "                df = df.dropna(subset=['SRTMGL3_DEM'])\n",
    "\n",
    "            # Continue only if there are remaining rows after filtering\n",
    "            if not df.empty:\n",
    "                # Add region or province column\n",
    "                df[region_column_name] = row[region_column_name]\n",
    "                df['district'] = row['district']  # assuming 'district' column always exists\n",
    "                results.append(df)\n",
    "            else:\n",
    "                print(f\"No Elevation data for geometry at index {idx}. Skipping...\")\n",
    "\n",
    "        else:\n",
    "            print(f\"No data for geometry at index {idx} in the dataset.\")\n",
    "\n",
    "    # Combine all dataframes into a single DataFrame\n",
    "    if results:\n",
    "        result_df = pd.concat(results, ignore_index=True)\n",
    "    else:\n",
    "        result_df = pd.DataFrame()  # Return an empty DataFrame if no results\n",
    "\n",
    "    # Compute statistics\n",
    "    stats_df = compute_stats(result_df, region_column_name)\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    csv_path = os.path.join(save_dir, 'district_elevation_stats.csv')\n",
    "    stats_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    # Merge statistics with the original GeoDataFrame\n",
    "    merged_gdf = gdf.merge(stats_df, on=['district', region_column_name])\n",
    "\n",
    "    # Save the spatial data with geometry\n",
    "    geojson_path = os.path.join(save_dir, 'district_elevation_stats.geojson')\n",
    "    merged_gdf.to_file(geojson_path, driver='GeoJSON')\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2eca8a",
   "metadata": {},
   "source": [
    "## Download Tanzania Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "806ac25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "geojson_file = 'tanzania_data/shapefiles/tz_country.geojson'\n",
    "start_date = '02-11-2000'\n",
    "end_date   = '02-21-2000'\n",
    "product_id = 'SRTMGL3_NC.003' \n",
    "layer_name = 'SRTMGL3_DEM'# 90m resolution layer\n",
    "\n",
    "dest_dir = \"tanzania_data/elevation_data/\"\n",
    "#submit_task_and_fetch_data(geojson_file, start_date, end_date, product_id, layer_name, dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b20744",
   "metadata": {},
   "source": [
    "## Extra Data for each Districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03188489",
   "metadata": {},
   "outputs": [],
   "source": [
    "tz_dir = 'tanzania_data/elevation_data/'\n",
    "nc_file = tz_dir+ '2000/2000_SRTMGL3_NC.003_90m_aid0001.nc'\n",
    "shapefile = 'tanzania_data/shapefiles/tz_districts.shp'\n",
    "output_directory = tz_dir + 'processed/'\n",
    "#extr_data = extract_elevation_data(nc_file, shapefile, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2a3d54",
   "metadata": {},
   "source": [
    "## Rwanda Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "988eafff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "geojson_file = 'rwanda_data/shapefiles/rw_country.geojson'\n",
    "start_date = '02-11-2000'\n",
    "end_date   = '02-21-2000'\n",
    "product_id = 'SRTMGL3_NC.003' \n",
    "layer_name = 'SRTMGL3_DEM'# 90m resolution layer\n",
    "dest_dir = \"rwanda_data/elevation_data/\"\n",
    "#submit_task_and_fetch_data(geojson_file, start_date, end_date, product_id, layer_name, dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c06d94",
   "metadata": {},
   "source": [
    "## Extra Data for each Districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51de8044",
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_dir = 'rwanda_data/elevation_data/'\n",
    "nc_file = rw_dir+ '2000/2000_SRTMGL3_NC.003_90m_aid0001.nc'\n",
    "shapefile = 'rwanda_data/shapefiles/rw_district.shp'\n",
    "output_directory = rw_dir + 'processed/'\n",
    "extr_data = extract_elevation_data(nc_file, shapefile, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0254d664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>crs</th>\n",
       "      <th>SRTMGL3_DEM</th>\n",
       "      <th>province</th>\n",
       "      <th>district</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-02-11 00:00:00</td>\n",
       "      <td>-1.308333</td>\n",
       "      <td>29.824167</td>\n",
       "      <td>0</td>\n",
       "      <td>2491.0</td>\n",
       "      <td>Amajyaruguru</td>\n",
       "      <td>Burera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-02-11 00:00:00</td>\n",
       "      <td>-1.309167</td>\n",
       "      <td>29.822500</td>\n",
       "      <td>0</td>\n",
       "      <td>2447.0</td>\n",
       "      <td>Amajyaruguru</td>\n",
       "      <td>Burera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-02-11 00:00:00</td>\n",
       "      <td>-1.309167</td>\n",
       "      <td>29.823333</td>\n",
       "      <td>0</td>\n",
       "      <td>2467.0</td>\n",
       "      <td>Amajyaruguru</td>\n",
       "      <td>Burera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-02-11 00:00:00</td>\n",
       "      <td>-1.309167</td>\n",
       "      <td>29.824167</td>\n",
       "      <td>0</td>\n",
       "      <td>2474.0</td>\n",
       "      <td>Amajyaruguru</td>\n",
       "      <td>Burera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-02-11 00:00:00</td>\n",
       "      <td>-1.309167</td>\n",
       "      <td>29.825000</td>\n",
       "      <td>0</td>\n",
       "      <td>2488.0</td>\n",
       "      <td>Amajyaruguru</td>\n",
       "      <td>Burera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time       lat        lon  crs  SRTMGL3_DEM      province  \\\n",
       "0  2000-02-11 00:00:00 -1.308333  29.824167    0       2491.0  Amajyaruguru   \n",
       "1  2000-02-11 00:00:00 -1.309167  29.822500    0       2447.0  Amajyaruguru   \n",
       "2  2000-02-11 00:00:00 -1.309167  29.823333    0       2467.0  Amajyaruguru   \n",
       "3  2000-02-11 00:00:00 -1.309167  29.824167    0       2474.0  Amajyaruguru   \n",
       "4  2000-02-11 00:00:00 -1.309167  29.825000    0       2488.0  Amajyaruguru   \n",
       "\n",
       "  district  \n",
       "0   Burera  \n",
       "1   Burera  \n",
       "2   Burera  \n",
       "3   Burera  \n",
       "4   Burera  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extr_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eedd3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
