{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0344f716",
   "metadata": {},
   "source": [
    "# Timeseries Data Collection and Preprocessing\n",
    "\n",
    "This notebook focuses on collecting and preprocessing time-series data relevant to food security indicators. It encompasses a diverse range of data sources, including temperature, rainfall, and food item prices. The collected data undergoes preprocessing to ensure its quality and suitability for predictive modeling of food security indicators. By using this comprehensive dataset, we aim to gain insights into the dynamic interplay between environmental factors, economic variables, and food security outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09addc81",
   "metadata": {},
   "source": [
    "## Data Sources\n",
    "\n",
    "In this section, we provide links to the diverse array of data sources utilized in our analysis. These sources encompass a wide range of domains crucial for understanding food security dynamics, including meteorological data repositories, governmental databases, and market price indices. By using these reputable and diverse sources, we ensure the richness and reliability of the data underpinning our analysis. These data are publicly available on the following data repository:\n",
    "\n",
    "* [NASA's POWER (Prediction Of Worldwide Energy Resources)](https://power.larc.nasa.gov/data-access-viewer/): \n",
    "This resource is part of NASA Earth Science's Applied Sciences Program, designed to support various energy-related applications by providing access to NASA's solar radiation and meteorological research data.The research areas are include in aids of **Renewable Energy**, and **Agricultural Needs**. It is also provides the [API](https://power.larc.nasa.gov/docs/services/api/) services for customs scripts and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0edae0f",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57801bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nasa-power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b7fb6bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import geopandas as gpd\n",
    "from geopandas import read_file\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import calendar\n",
    "from shapely.ops import cascaded_union\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, Polygon\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393172d7",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0373a830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids(shapefile_path, region_col, district_col):\n",
    "    \n",
    "    '''\n",
    "    Compute centroids of districts in a shapefile.\n",
    "\n",
    "    Args:\n",
    "    - shapefile_path (str): Path to the shapefile.\n",
    "    - region_col (str): Name of the column containing region or province information.\n",
    "    - district_col (str): Name of the column containing district information.\n",
    "\n",
    "    Returns:\n",
    "    - centroid_df (GeoDataFrame): DataFrame containing region, district,\n",
    "      centroid lat, and centroid lon.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Read the shapefile\n",
    "    gdf = read_file(shapefile_path)\n",
    "    \n",
    "    # Ensure the columns exist in the DataFrame\n",
    "    if region_col not in gdf.columns or district_col not in gdf.columns:\n",
    "        raise ValueError(\"Columns not found in the shapefile.\")\n",
    "    \n",
    "    # Compute centroids\n",
    "    centroids = gdf.centroid\n",
    "    \n",
    "    # Extract centroid coordinates\n",
    "    centroid_coords = centroids.apply(lambda x: (x.y, x.x))\n",
    "    \n",
    "    # Create a DataFrame with region, district, and centroid coordinates\n",
    "    centroid_df = gpd.GeoDataFrame({\n",
    "        'region': gdf[region_col],\n",
    "        'district': gdf[district_col],\n",
    "        'centroid_lat': centroid_coords.apply(lambda x: x[0]),\n",
    "        'centroid_lon': centroid_coords.apply(lambda x: x[1])\n",
    "    })\n",
    "    \n",
    "    return centroid_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "305e615d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_district_bounds(shapefile_path):\n",
    "    \n",
    "    '''\n",
    "    Compute the bounds (minimum and maximum longitude and latitude) of districts in a shapefile.\n",
    "\n",
    "    Args:\n",
    "    - shapefile_path (str): Path to the shapefile.\n",
    "\n",
    "    Returns:\n",
    "    - district_bounds (DataFrame): DataFrame containing region, district,\n",
    "      minimum longitude, maximum longitude, minimum latitude, and maximum latitude.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Read the shapefile\n",
    "    gdf = read_file(shapefile_path)\n",
    "\n",
    "    # Group by district and compute the maximum and minimum longitude and latitude\n",
    "    district_bounds = gdf.groupby(['region', 'district'])['geometry'].apply(lambda x: x.bounds).reset_index()\n",
    "\n",
    "    # Extract maximum and minimum longitude and latitude values\n",
    "    district_bounds['min_lon'] = district_bounds['minx']\n",
    "    district_bounds['max_lon'] = district_bounds['maxx']\n",
    "    district_bounds['min_lat'] = district_bounds['miny']\n",
    "    district_bounds['max_lat'] = district_bounds['maxy']\n",
    "\n",
    "    # Select relevant columns\n",
    "    district_bounds = district_bounds[['region', 'district', 'min_lon', 'max_lon', 'min_lat', 'max_lat']]\n",
    "\n",
    "    return district_bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0333a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_points_in_polygon(polygon, num_points, min_distance=2):\n",
    "    '''\n",
    "    Generate random points inside a polygon with a minimum distance between points.\n",
    "\n",
    "    Parameters:\n",
    "        polygon (Polygon): Polygon object representing the district boundary.\n",
    "        num_points (int): Number of random points to generate.\n",
    "        min_distance (float): Minimum distance between points in kilometers.\n",
    "\n",
    "    Returns:\n",
    "        list: List of Point objects representing the random points inside the polygon.\n",
    "    '''\n",
    "    points = []\n",
    "    min_x, min_y, max_x, max_y = polygon.bounds\n",
    "    boundary_polygon = polygon.buffer(0)\n",
    "    while len(points) < num_points:\n",
    "        point = Point(np.random.uniform(min_x, max_x), np.random.uniform(min_y, max_y))\n",
    "        if point.within(boundary_polygon):\n",
    "            if all(point.distance(existing_point) > min_distance for existing_point in points):\n",
    "                points.append(point)\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ce6814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_district_points(shapefile_path, num_points_per_district, min_distance=2):\n",
    "    '''\n",
    "    Compute random points inside each district polygon in a shapefile with a minimum distance between points.\n",
    "\n",
    "    Args:\n",
    "        shapefile_path (str): Path to the shapefile.\n",
    "        num_points_per_district (int): Number of random points to generate per district.\n",
    "        min_distance (float): Minimum distance between points in kilometers.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame containing region, district,\n",
    "                   longitude, and latitude of each random point.\n",
    "    '''\n",
    "    # Read the shapefile\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "    # Initialize an empty list to store district points\n",
    "    district_points = []\n",
    "\n",
    "    # Iterate over each district\n",
    "    for index, row in gdf.iterrows():\n",
    "        district = row['district']\n",
    "        region = row['region']\n",
    "        polygon = row['geometry']\n",
    "\n",
    "        # Generate random points inside the polygon with minimum distance between points\n",
    "        points = generate_random_points_in_polygon(polygon, num_points_per_district, min_distance)\n",
    "\n",
    "        # Extract longitude and latitude of each point\n",
    "        for point in points:\n",
    "            district_points.append({\n",
    "                'region': region,\n",
    "                'district': district,\n",
    "                'longitude': point.x,\n",
    "                'latitude': point.y\n",
    "            })\n",
    "\n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    district_points_df = pd.DataFrame(district_points)\n",
    "\n",
    "    return district_points_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21fcb0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_centroids(shapefile_path, region_col, district_col):\n",
    "    \n",
    "    '''\n",
    "    Visualize centroids of districts on a map.\n",
    "\n",
    "    Args:\n",
    "    - shapefile_path (str): Path to the shapefile.\n",
    "    - region_col (str): Name of the column containing region or province information.\n",
    "    - district_col (str): Name of the column containing district information.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Compute centroids using compute_centroids function\n",
    "    centroid_df = compute_centroids(shapefile_path, region_col, district_col)\n",
    "\n",
    "    # Read the shapefile\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "    # Plot districts\n",
    "    ax = gdf.plot(figsize=(10, 10), edgecolor='black', alpha=0.5)\n",
    "\n",
    "    # Plot centroids\n",
    "    ax.scatter(centroid_df['centroid_lon'], centroid_df['centroid_lat'], color='red', s=50, label='Centroids')\n",
    "\n",
    "    # Add labels\n",
    "    for x, y, label in zip(centroid_df['centroid_lon'], centroid_df['centroid_lat'], centroid_df[district_col]):\n",
    "        ax.text(x, y, label, fontsize=10, ha='center', va='center')\n",
    "\n",
    "    # Set plot title\n",
    "    plt.title('Centroids of Districts')\n",
    "\n",
    "    # Set x and y axis labels\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "\n",
    "    # Show legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db3cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data(df, groupby_columns, mean_columns):\n",
    "    \n",
    "    '''\n",
    "    Aggregate data in a DataFrame based on specified groupby columns and calculate the mean of specified columns.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): Input DataFrame containing the data.\n",
    "        groupby_columns (list): List of columns to group by.\n",
    "        mean_columns (list): List of columns for which the mean needs to be calculated.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame containing the aggregated data with means of specified columns.\n",
    "    '''\n",
    "    # Prepare dictionary with mean aggregation for specified columns\n",
    "    aggregation_dict = {column: 'mean' for column in mean_columns}\n",
    "\n",
    "    # Group by specified columns and calculate the mean\n",
    "    aggregated_df = df.groupby(groupby_columns).agg(aggregation_dict).reset_index()\n",
    "\n",
    "    return aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd18413c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_district_points\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtanzania_data/tz_shapefiles/tz_districts.shp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 27\u001b[0m, in \u001b[0;36mcompute_district_points\u001b[1;34m(shapefile_path, num_points_per_district, min_distance)\u001b[0m\n\u001b[0;32m     24\u001b[0m polygon \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Generate random points inside the polygon with minimum distance between points\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m points \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_random_points_in_polygon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolygon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_points_per_district\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_distance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Extract longitude and latitude of each point\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m point \u001b[38;5;129;01min\u001b[39;00m points:\n",
      "Cell \u001b[1;32mIn[6], line 18\u001b[0m, in \u001b[0;36mgenerate_random_points_in_polygon\u001b[1;34m(polygon, num_points, min_distance)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(points) \u001b[38;5;241m<\u001b[39m num_points:\n\u001b[0;32m     17\u001b[0m     point \u001b[38;5;241m=\u001b[39m Point(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(min_x, max_x), np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(min_y, max_y))\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboundary_polygon\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(point\u001b[38;5;241m.\u001b[39mdistance(existing_point) \u001b[38;5;241m>\u001b[39m min_distance \u001b[38;5;28;01mfor\u001b[39;00m existing_point \u001b[38;5;129;01min\u001b[39;00m points):\n\u001b[0;32m     20\u001b[0m             points\u001b[38;5;241m.\u001b[39mappend(point)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\geo\\lib\\site-packages\\shapely\\geometry\\base.py:714\u001b[0m, in \u001b[0;36mBaseGeometry.within\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwithin\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m    713\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns True if geometry is within the other, else False\"\"\"\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _maybe_unpack(\u001b[43mshapely\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\geo\\lib\\site-packages\\shapely\\decorators.py:77\u001b[0m, in \u001b[0;36mmultithreading_enabled.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m array_args:\n\u001b[0;32m     76\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arr, old_flag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(array_args, old_flags):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\geo\\lib\\site-packages\\shapely\\predicates.py:946\u001b[0m, in \u001b[0;36mwithin\u001b[1;34m(a, b, **kwargs)\u001b[0m\n\u001b[0;32m    897\u001b[0m \u001b[38;5;129m@multithreading_enabled\u001b[39m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwithin\u001b[39m(a, b, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    899\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns True if geometry A is completely inside geometry B.\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \n\u001b[0;32m    901\u001b[0m \u001b[38;5;124;03m    A is within B if no points of A lie in the exterior of B and at least one\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 946\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithin\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = compute_district_points('tanzania_data/tz_shapefiles/tz_districts.shp', 20, min_distance=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c90d3c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc7d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('tanzania_data/tz_districts_points.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9dbd186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize_centroids('tanzania_data/tz_shapefiles/tz_districts.shp','region','district')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9957af78",
   "metadata": {},
   "source": [
    "## Collecting Temperature Data\n",
    "Understanding temperature patterns is crucial for agriculture as it impacts various aspects of crop growth, pest management, and water availability. By analyzing temperature trends over time, we can assess the vulnerability of agricultural systems to climate fluctuations and long-term changes which is essential factor for addessing food insecurity and improve agricultural resilience in the context of evolving climate conditions.Therefore in this section, we will be using [NASA's Power API](https://power.larc.nasa.gov/docs/services/api/temporal/monthly/) services to gather comprehensive temperature datasets spanning diverse geographic regions and temporal scales of monthly base. It provides parameters by year; the annual and each month's average, maximum, and/or minimum values. These data can be collected by single point and by regional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f7e267",
   "metadata": {},
   "source": [
    "### Single Point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90170639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_temperature_data_point(lat, lon):\n",
    "    \n",
    "    '''\n",
    "    Fetches temperature data from the NASA POWER API for a given latitude and longitude.\n",
    "\n",
    "    Parameters:\n",
    "        lat (float): Latitude of the location.\n",
    "        lon (float): Longitude of the location.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the fetched data if successful, None otherwise.\n",
    "    '''\n",
    "    \n",
    "    parameters = {\n",
    "        'latitude': lat,\n",
    "        'longitude': lon,\n",
    "        'start': '1990',         # Start year (YYYY)\n",
    "        'end': '2022',           # End year (YYYY)\n",
    "        'community': 'AG',       # Agroclimatology Archive\n",
    "        'parameters': ','.join([\n",
    "            'T2M',                # MERRA-2 Temperature at 2 Meters (C)\n",
    "            'T2M_MAX',            # MERRA-2 Temperature at 2 Meters Maximum (C)\n",
    "            'T2M_MIN',            # MERRA-2 Temperature at 2 Meters Minimum (C)\n",
    "        ]),\n",
    "        'format': 'json',        # Data format (json or csv)\n",
    "        'temporalAverage': 'monthly'  # Temporal resolution (monthly)\n",
    "    }\n",
    "\n",
    "    # Make a GET request to the NASA POWER API\n",
    "    response = requests.get('https://power.larc.nasa.gov/api/temporal/monthly/point', params=parameters)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract and return the data from the response\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(\"Failed to retrieve data. Status code:\", response.status_code)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8536a5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_point_temperature(centroids_df):\n",
    "    '''\n",
    "    Calculates temperature data for each district centroid and returns a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        centroids_df (DataFrame): DataFrame containing centroids (latitude and longitude) of each district.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing the calculated temperature data for each district centroid.\n",
    "    '''\n",
    "    # Initialize an empty list to store individual DataFrames\n",
    "    all_data_frames = []\n",
    "\n",
    "    # Loop over each row in the DataFrame\n",
    "    for index, row in centroids_df.iterrows():\n",
    "        latitude = row['centroid_lat']\n",
    "        longitude = row['centroid_lon']\n",
    "        district = row['district']\n",
    "        region = row['region']\n",
    "\n",
    "        # Fetch NASA data\n",
    "        nasa_data = fetch_temperature_data_point(latitude, longitude)\n",
    "\n",
    "        if nasa_data:\n",
    "            # Convert the response to a pandas DataFrame\n",
    "            nasa_df = pd.DataFrame(nasa_data['properties']['parameter'])\n",
    "\n",
    "            # Extract year and month from the index\n",
    "            nasa_df.index = nasa_df.index.astype(str)  # Convert index to string\n",
    "            nasa_df['year'] = nasa_df.index.str[:4]    # Extract first 4 characters as year\n",
    "            nasa_df['month'] = nasa_df.index.str[4:]   # Extract remaining characters as month\n",
    "            nasa_df['district'] = district\n",
    "            nasa_df['region'] = region\n",
    "\n",
    "            # Reorder columns\n",
    "            nasa_df = nasa_df[['region', 'district', 'year', 'month', 'T2M', 'T2M_MAX', 'T2M_MIN']]\n",
    "\n",
    "            # Append DataFrame to the list\n",
    "            all_data_frames.append(nasa_df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    all_data = pd.concat(all_data_frames, ignore_index=True)\n",
    "\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4cf5d2",
   "metadata": {},
   "source": [
    "### Regional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68101fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_temperature_data_region(lat_min, lat_max, lon_min, lon_max):\n",
    "    \n",
    "    '''\n",
    "    Fetches temperature data for a given region from the NASA POWER API.\n",
    "\n",
    "    Parameters:\n",
    "        lat_min (float): The minimum latitude of the region.\n",
    "        lat_max (float): The maximum latitude of the region.\n",
    "        lon_min (float): The minimum longitude of the region.\n",
    "        lon_max (float): The maximum longitude of the region.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the fetched temperature data, or None if the request failed.\n",
    "    '''\n",
    "    parameters = {\n",
    "        'latitude-min': lat_min,\n",
    "        'latitude-max': lat_max,\n",
    "        'longitude-min': lon_min,\n",
    "        'longitude-max': lon_max,\n",
    "        'start': 1990,  # Start year (YYYY)\n",
    "        'end': 2020,    # End year (YYYY)\n",
    "        'community': 'ag',    # Agroclimatology Archive\n",
    "        'parameters': ','.join([\n",
    "            'T2M',                 # MERRA-2 Temperature at 2 Meters (C)\n",
    "            'T2M_MAX',             # MERRA-2 Temperature at 2 Meters Maximum (C)\n",
    "            'T2M_MIN',             # MERRA-2 Temperature at 2 Meters Minimum (C)\n",
    "        ]),\n",
    "        'format': 'json',       # Data format (json or csv)\n",
    "        'temporalAverage': 'monthly'  # Temporal resolution (monthly)\n",
    "    }\n",
    "\n",
    "    # Make a GET request to the NASA POWER API\n",
    "    response = requests.get('https://power.larc.nasa.gov/api/temporal/monthly/regional', params=parameters)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract and return the data from the response\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(\"Failed to retrieve data. Status code:\", response.status_code)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70b8bb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_regional_temperature(district_bounds_df):\n",
    "    '''\n",
    "    Calculates temperature data for each district region and returns a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        district_bounds_df (DataFrame): DataFrame containing district bounds.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing the calculated temperature data for each district region.\n",
    "    '''\n",
    "    # Initialize an empty list to store individual DataFrames\n",
    "    all_data_frames = []\n",
    "\n",
    "    # Loop over each row in the DataFrame\n",
    "    for index, row in district_bounds_df.iterrows():\n",
    "        min_lat = row['min_lat']\n",
    "        max_lat = row['max_lat']\n",
    "        min_lon = row['min_lon']\n",
    "        max_lon = row['max_lon']\n",
    "        district = row['district']\n",
    "        region = row['region']\n",
    "\n",
    "        # Fetch NASA data\n",
    "        nasa_data = fetch_temperature_data_region(min_lat, max_lat, min_lon, max_lon)\n",
    "\n",
    "        if nasa_data:\n",
    "            # Convert the response to a pandas DataFrame\n",
    "            nasa_df = pd.DataFrame(nasa_data['properties']['parameter'])\n",
    "\n",
    "            # Extract year and month from the index\n",
    "            nasa_df.index = nasa_df.index.astype(str)  # Convert index to string\n",
    "            nasa_df['year'] = nasa_df.index.str[:4]    # Extract first 4 characters as year\n",
    "            nasa_df['month'] = nasa_df.index.str[4:]   # Extract remaining characters as month\n",
    "            nasa_df['district'] = district\n",
    "            nasa_df['region'] = region\n",
    "\n",
    "            # Reorder columns\n",
    "            nasa_df = nasa_df[['region', 'district', 'year', 'month', 'T2M', 'T2M_MAX', 'T2M_MIN']]\n",
    "\n",
    "            # Append DataFrame to the list\n",
    "            all_data_frames.append(nasa_df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    all_data = pd.concat(all_data_frames, ignore_index=True)\n",
    "\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f67ed9",
   "metadata": {},
   "source": [
    "### TANZANIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e5a8618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashas\\AppData\\Local\\Temp\\ipykernel_11536\\341830614.py:25: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroids = gdf.centroid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>district</th>\n",
       "      <th>centroid_lat</th>\n",
       "      <th>centroid_lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arusha</td>\n",
       "      <td>Arusha</td>\n",
       "      <td>-3.347952</td>\n",
       "      <td>36.688170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arusha</td>\n",
       "      <td>Arusha Urban</td>\n",
       "      <td>-3.437964</td>\n",
       "      <td>36.675364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arusha</td>\n",
       "      <td>Karatu</td>\n",
       "      <td>-3.554529</td>\n",
       "      <td>35.435120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arusha</td>\n",
       "      <td>Lake Eyasi</td>\n",
       "      <td>-3.585394</td>\n",
       "      <td>35.109951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arusha</td>\n",
       "      <td>Lake Manyara</td>\n",
       "      <td>-3.515286</td>\n",
       "      <td>35.836992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   region      district  centroid_lat  centroid_lon\n",
       "0  Arusha        Arusha     -3.347952     36.688170\n",
       "1  Arusha  Arusha Urban     -3.437964     36.675364\n",
       "2  Arusha        Karatu     -3.554529     35.435120\n",
       "3  Arusha    Lake Eyasi     -3.585394     35.109951\n",
       "4  Arusha  Lake Manyara     -3.515286     35.836992"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#computing The Centroids of Distrcits\n",
    "district_centroids = compute_centroids('tanzania_data/tz_shapefiles/tz_districts.shp','region','district')\n",
    "district_centroids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ae4e68ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>district</th>\n",
       "      <th>min_lon</th>\n",
       "      <th>max_lon</th>\n",
       "      <th>min_lat</th>\n",
       "      <th>max_lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arusha</td>\n",
       "      <td>Arusha</td>\n",
       "      <td>36.469810</td>\n",
       "      <td>36.889061</td>\n",
       "      <td>-3.655550</td>\n",
       "      <td>-3.066394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arusha</td>\n",
       "      <td>Arusha Urban</td>\n",
       "      <td>36.583969</td>\n",
       "      <td>36.767124</td>\n",
       "      <td>-3.558246</td>\n",
       "      <td>-3.342853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arusha</td>\n",
       "      <td>Karatu</td>\n",
       "      <td>34.750038</td>\n",
       "      <td>35.976353</td>\n",
       "      <td>-3.946814</td>\n",
       "      <td>-3.032225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arusha</td>\n",
       "      <td>Lake Eyasi</td>\n",
       "      <td>34.788990</td>\n",
       "      <td>35.355385</td>\n",
       "      <td>-3.825978</td>\n",
       "      <td>-3.352262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arusha</td>\n",
       "      <td>Lake Manyara</td>\n",
       "      <td>35.766632</td>\n",
       "      <td>35.891113</td>\n",
       "      <td>-3.625842</td>\n",
       "      <td>-3.421769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   region      district    min_lon    max_lon   min_lat   max_lat\n",
       "0  Arusha        Arusha  36.469810  36.889061 -3.655550 -3.066394\n",
       "1  Arusha  Arusha Urban  36.583969  36.767124 -3.558246 -3.342853\n",
       "2  Arusha        Karatu  34.750038  35.976353 -3.946814 -3.032225\n",
       "3  Arusha    Lake Eyasi  34.788990  35.355385 -3.825978 -3.352262\n",
       "4  Arusha  Lake Manyara  35.766632  35.891113 -3.625842 -3.421769"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#csv_data = fetch_temperature_data_point(latitude, longitude)\n",
    "#csv_data=fetch_temperature_data_point(-3.347952, 36.688170)\n",
    "districts_bounds = compute_district_bounds('tanzania_data/tz_shapefiles/tz_districts.shp')\n",
    "districts_bounds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ab03950",
   "metadata": {},
   "outputs": [],
   "source": [
    "if csv_data:\n",
    "        # Convert the response to a pandas DataFrame\n",
    "        nasa_df = pd.DataFrame(csv_data['properties']['parameter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f2b9559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['T2M', 'T2M_MAX', 'T2M_MIN'], dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nasa_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb08ea88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['199001', '199002', '199003', '199004', '199005', '199006', '199007',\n",
       "       '199008', '199009', '199010',\n",
       "       ...\n",
       "       '202204', '202205', '202206', '202207', '202208', '202209', '202210',\n",
       "       '202211', '202212', '202213'],\n",
       "      dtype='object', length=429)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nasa_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6d2d1bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch the data\n",
    "#tz_temperature = compute_point_temperature(district_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9dcba001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>district</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>T2M</th>\n",
       "      <th>T2M_MAX</th>\n",
       "      <th>T2M_MIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arusha</td>\n",
       "      <td>Arusha</td>\n",
       "      <td>1990</td>\n",
       "      <td>01</td>\n",
       "      <td>22.65</td>\n",
       "      <td>32.57</td>\n",
       "      <td>14.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arusha</td>\n",
       "      <td>Arusha</td>\n",
       "      <td>1990</td>\n",
       "      <td>02</td>\n",
       "      <td>23.51</td>\n",
       "      <td>35.32</td>\n",
       "      <td>16.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arusha</td>\n",
       "      <td>Arusha</td>\n",
       "      <td>1990</td>\n",
       "      <td>03</td>\n",
       "      <td>21.68</td>\n",
       "      <td>32.15</td>\n",
       "      <td>15.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arusha</td>\n",
       "      <td>Arusha</td>\n",
       "      <td>1990</td>\n",
       "      <td>04</td>\n",
       "      <td>20.70</td>\n",
       "      <td>27.09</td>\n",
       "      <td>15.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arusha</td>\n",
       "      <td>Arusha</td>\n",
       "      <td>1990</td>\n",
       "      <td>05</td>\n",
       "      <td>19.55</td>\n",
       "      <td>27.15</td>\n",
       "      <td>13.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Arusha</td>\n",
       "      <td>Arusha</td>\n",
       "      <td>1990</td>\n",
       "      <td>06</td>\n",
       "      <td>18.64</td>\n",
       "      <td>28.42</td>\n",
       "      <td>11.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Arusha</td>\n",
       "      <td>Arusha</td>\n",
       "      <td>1990</td>\n",
       "      <td>07</td>\n",
       "      <td>17.95</td>\n",
       "      <td>27.48</td>\n",
       "      <td>9.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Arusha</td>\n",
       "      <td>Arusha</td>\n",
       "      <td>1990</td>\n",
       "      <td>08</td>\n",
       "      <td>19.59</td>\n",
       "      <td>29.85</td>\n",
       "      <td>12.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Arusha</td>\n",
       "      <td>Arusha</td>\n",
       "      <td>1990</td>\n",
       "      <td>09</td>\n",
       "      <td>21.35</td>\n",
       "      <td>32.05</td>\n",
       "      <td>13.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Arusha</td>\n",
       "      <td>Arusha</td>\n",
       "      <td>1990</td>\n",
       "      <td>10</td>\n",
       "      <td>22.27</td>\n",
       "      <td>32.19</td>\n",
       "      <td>13.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Arusha</td>\n",
       "      <td>Arusha</td>\n",
       "      <td>1990</td>\n",
       "      <td>11</td>\n",
       "      <td>22.47</td>\n",
       "      <td>32.40</td>\n",
       "      <td>16.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Arusha</td>\n",
       "      <td>Arusha</td>\n",
       "      <td>1990</td>\n",
       "      <td>12</td>\n",
       "      <td>22.80</td>\n",
       "      <td>31.83</td>\n",
       "      <td>16.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Arusha</td>\n",
       "      <td>Arusha</td>\n",
       "      <td>1990</td>\n",
       "      <td>13</td>\n",
       "      <td>21.08</td>\n",
       "      <td>35.32</td>\n",
       "      <td>9.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Arusha</td>\n",
       "      <td>Arusha</td>\n",
       "      <td>1991</td>\n",
       "      <td>01</td>\n",
       "      <td>24.19</td>\n",
       "      <td>34.34</td>\n",
       "      <td>15.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    region district  year month    T2M  T2M_MAX  T2M_MIN\n",
       "0   Arusha   Arusha  1990    01  22.65    32.57    14.08\n",
       "1   Arusha   Arusha  1990    02  23.51    35.32    16.51\n",
       "2   Arusha   Arusha  1990    03  21.68    32.15    15.95\n",
       "3   Arusha   Arusha  1990    04  20.70    27.09    15.88\n",
       "4   Arusha   Arusha  1990    05  19.55    27.15    13.61\n",
       "5   Arusha   Arusha  1990    06  18.64    28.42    11.02\n",
       "6   Arusha   Arusha  1990    07  17.95    27.48     9.75\n",
       "7   Arusha   Arusha  1990    08  19.59    29.85    12.51\n",
       "8   Arusha   Arusha  1990    09  21.35    32.05    13.76\n",
       "9   Arusha   Arusha  1990    10  22.27    32.19    13.47\n",
       "10  Arusha   Arusha  1990    11  22.47    32.40    16.07\n",
       "11  Arusha   Arusha  1990    12  22.80    31.83    16.36\n",
       "12  Arusha   Arusha  1990    13  21.08    35.32     9.75\n",
       "13  Arusha   Arusha  1991    01  24.19    34.34    15.94"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tz_temperature.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9f9b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined monthly temperature data to a CSV file\n",
    "tz_temperature.to_csv('tanzania_data/tz_monthly_temperature_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a862e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
